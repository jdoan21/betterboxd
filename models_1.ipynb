{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "models_1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "e6NKucoOZIUL",
        "ZVeCw4XTZZDN",
        "0pfenmQm5-Lc",
        "T-In4ARN6ZcF",
        "bDzU3872EaUn",
        "QtshuuOqEgCe",
        "X0uNfyHKG5W-",
        "7KUrqw9V_tOS"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j02i7xgd3tUQ"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTGoh-HMY6GL",
        "outputId": "04366bb9-158f-4b3f-8b9b-9ed8c005b5e7"
      },
      "source": [
        "# this mounts your Google Drive to the Colab VM.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "\n",
        "FOLDERNAME = 'cs229_proj/'\n",
        "\n",
        "\n",
        "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
        "\n",
        "# now that we've mounted your Drive, this ensures that\n",
        "# the Python interpreter of the Colab VM can load\n",
        "# python files from within it.\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n",
        "\n",
        "%cd drive/My\\ Drive/$FOLDERNAME/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/cs229_proj\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0jdgSEUY-oR"
      },
      "source": [
        "!pwd\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqnsJQB3Y_zi"
      },
      "source": [
        "from IPython.display import Image\n",
        "import json\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "import torch\n",
        "from torch import nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5m4bttkk-RHC"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.impute import SimpleImputer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ep0kH1OOZKx6"
      },
      "source": [
        "# Load Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFIwGhrEZJ5g"
      },
      "source": [
        "movies_full_path = '/content/drive/My Drive/cs229_proj/movies_full.csv'\n",
        "user_ratings_full_path = '/content/drive/My Drive/cs229_proj/user_ratings_full.csv'\n",
        "movies_embeddings_full_path = '/content/drive/My Drive/cs229_proj/movies_embeddings_full.csv'\n",
        "movies_svd_path = '/content/drive/My Drive/cs229_proj/movies_svd_full.csv'\n",
        "\n",
        "movies_full_df = pd.read_csv(movies_full_path)\n",
        "ratings_full_df = pd.read_csv(user_ratings_full_path)\n",
        "movies_embeddings_df = pd.read_csv(movies_embeddings_full_path)\n",
        "movies_svd_df = pd.read_csv(movies_svd_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ADfW30E8DF1",
        "outputId": "6f23a447-1656-4d1a-a777-a4b0472a476e"
      },
      "source": [
        "print(movies_full_df.shape)\n",
        "print(ratings_full_df.shape)\n",
        "print(movies_embeddings_df.shape)\n",
        "print(movies_svd_df.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2220, 55)\n",
            "(392551, 3)\n",
            "(2220, 1567)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OBgwQceZOff",
        "outputId": "b750550a-1c7f-49b7-d154-733e58a1148b"
      },
      "source": [
        "# print(movies_full_df.columns)\n",
        "\n",
        "all_columns = ['movie_id', 'poster_path', 'title',\n",
        "       'year', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10',\n",
        "       'tmdb_budget', 'imdb_budget', 'tmdb_revenue', 'imdb_revenue',\n",
        "       'tmdb_vote_average', 'tmdb_vote_count', 'imdb_vote_average',\n",
        "       'imdb_vote_count', 'tmdb_popularity', 'tmdb_runtime', 'imdb_runtime',\n",
        "       'main_genre', 'genres', 'director', 'writer', 'main_actor',\n",
        "       'mpaa_rating', 'overview', 'tagline', 'keywords', 'release_date',\n",
        "       'main_prod_company', 'production_companies', 'country',\n",
        "       'production_countries', 'original_language', 'spoken_languages',\n",
        "       'rt_info', 'critics_consensus', 'actors', 'tm_status', 'tm_rating',\n",
        "       'tm_count', 'audience_status', 'audience_rating', 'audience_count',\n",
        "       'tm_top_critics_count', 'tm_fresh_critics_count',\n",
        "       'tm_rotten_critics_count', 'ebert_rating', 'boxd_vote_average']\n",
        "print(len(all_columns))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "55\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UixwixmN_-8X"
      },
      "source": [
        "# Features (and data preprocessing)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUvC2gAgZgHU",
        "outputId": "a96f3c73-e46d-4610-b04c-e264ecb6d6c9"
      },
      "source": [
        "# All features split up into many categories\n",
        "\n",
        "regression_features = ['year', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10',\n",
        "       'tmdb_budget', 'imdb_budget', 'tmdb_revenue', 'imdb_revenue', 'tmdb_vote_average', 'tmdb_vote_count', 'imdb_vote_average',\n",
        "       'imdb_vote_count', 'tmdb_popularity', 'tmdb_runtime', 'imdb_runtime', 'tm_rating',\n",
        "       'tm_count', 'audience_rating', 'audience_count',\n",
        "       'tm_top_critics_count', 'tm_fresh_critics_count',\n",
        "       'tm_rotten_critics_count', 'ebert_rating', 'boxd_vote_average']\n",
        "\n",
        "classification_features = ['main_genre', 'mpaa_rating', 'country', 'main_prod_company', 'tm_status', 'audience_status', \n",
        "                           'original_language']\n",
        "\n",
        "complex_features = ['genres', 'director', 'writer', 'main_actor', 'overview', 'tagline', 'keywords', 'release_date', \n",
        "                    'production_companies', 'production_countries', 'spoken_languages', 'rt_info', 'critics_consensus', \n",
        "                    'actors']\n",
        "\n",
        "other_features = ['movie_id', 'poster_path', 'title']\n",
        "\n",
        "print(len(regression_features) + len(classification_features) + len(complex_features) + len(other_features))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "55\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MwP_D2ACAY6"
      },
      "source": [
        "# Remove rows that have at least one NaN value (to make LinReg easier)\n",
        "def remove_missing_values(X):\n",
        "    old_len = X.shape[0]\n",
        "    X = X[~np.isnan(X).any(axis=1)]\n",
        "    new_len = X.shape[0]\n",
        "    print(\"Removed \" + str(old_len - new_len) + \" out of \" + str(old_len) + \" elements or \" + str((old_len - new_len)/old_len*100) + \"%\")\n",
        "    print(str(new_len) + \" elements remain\")\n",
        "    return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4NouGHbsPMi"
      },
      "source": [
        "def get_num_missing(X):\n",
        "    old_len = X.shape[0]\n",
        "    X = X[~np.isnan(X).any(axis=1)]\n",
        "    new_len = X.shape[0]\n",
        "    return old_len - new_len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGVq7gUgEjUK"
      },
      "source": [
        "def train_test_split(X, y, frac=0.8, verbose=False):\n",
        "    train_len = int(X.shape[0] * frac)\n",
        "    X_train = X[:train_len, :]\n",
        "    y_train = y[:train_len]\n",
        "    X_test = X[train_len:, :]\n",
        "    y_test = y[train_len:]\n",
        "    if verbose:\n",
        "        print(\"X_train:\", X_train.shape)\n",
        "        print(\"y_train:\", y_train.shape)\n",
        "        print(\"X_test:\", X_test.shape)\n",
        "        print(\"y_test:\", y_test.shape)\n",
        "    return X_train, y_train, X_test, y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yvAo50B--T6"
      },
      "source": [
        "# Models (for predicting average Letterboxd rating)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKZEr59b_DIG"
      },
      "source": [
        "## Multi-Layer Perceptron (Regression Features Only)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXFyj5r2_CXD"
      },
      "source": [
        "# No embeddings\n",
        "\n",
        "feature_lst = ['year', 'tmdb_budget', 'imdb_budget', 'tmdb_revenue', 'imdb_revenue', 'tmdb_runtime', 'imdb_runtime', \n",
        "                     'tm_rating', 'tm_count', 'tm_top_critics_count', 'tm_fresh_critics_count', \n",
        "                     'tm_rotten_critics_count', 'ebert_rating']\n",
        "\n",
        "output_label = 'boxd_vote_average'\n",
        "\n",
        "X_all = movies_full_df[feature_lst + [output_label]]\n",
        "\n",
        "print(get_num_missing(X_all))\n",
        "\n",
        "X_noembed = remove_missing_values(X_all)\n",
        "\n",
        "print(X_noembed.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzcKOZGi_Cb1"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "features_noembed = X_noembed[feature_lst].to_numpy()\n",
        "outputs_noembed = X_noembed[output_label].to_numpy()\n",
        "X_ne, y_ne = shuffle(features_noembed, outputs_noembed, random_state=1)\n",
        "print(X_ne.shape)\n",
        "print(y_ne.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEfCdXhuI0az"
      },
      "source": [
        "X_ne_train, y_ne_train, X_ne_test, y_ne_test = train_test_split(X_ne, y_ne,frac=0.9, verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKE8OC4XI0c_"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def get_rmse(y_pred, y_true):\n",
        "    return np.sqrt(np.mean((y_pred-y_true)**2))\n",
        "\n",
        "#this model learns to minimize MAE\n",
        "def mae_loss(y_pred, y_true):\n",
        "    mae = torch.abs(y_true - y_pred).mean()\n",
        "    return mae\n",
        "\n",
        "#this model learns to minimize RMSE\n",
        "def rmse_loss(y_pred, y_true):\n",
        "    return torch.sqrt(torch.mean((y_pred-y_true)**2))\n",
        "\n",
        "def get_train_test_datasets(X_train, y_train, X_test, y_test):\n",
        "    return MovieDataset(X_train, y_train), X_train, y_train, torch.tensor(X_test), torch.tensor(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vo8cMaWkI0fu"
      },
      "source": [
        "class MovieDataset(torch.utils.data.Dataset):\n",
        "  '''\n",
        "  Prepare the Movie dataset for regression\n",
        "  '''\n",
        "\n",
        "  def __init__(self, X, y, scale_data=True):\n",
        "    if not torch.is_tensor(X) and not torch.is_tensor(y):\n",
        "      # Apply scaling if necessary\n",
        "      if scale_data:\n",
        "          X = StandardScaler().fit_transform(X)\n",
        "      self.X = torch.from_numpy(X)\n",
        "      self.y = torch.from_numpy(y)\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.X)\n",
        "\n",
        "  def __getitem__(self, i):\n",
        "      return self.X[i], self.y[i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUIv0s1ZJb2B"
      },
      "source": [
        "class MLP(nn.Module):\n",
        "  '''\n",
        "    Multilayer Perceptron for regression.\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "      nn.Linear(13, 64),\n",
        "      nn.Dropout(),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(64, 32),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(32, 1)\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''\n",
        "      Forward pass\n",
        "    '''\n",
        "    return self.layers(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCseaC5YJb4V",
        "outputId": "91bc2c5d-c4b7-43a3-ddda-80403eab5b41"
      },
      "source": [
        "# Set fixed random number seed\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Prepare dataset\n",
        "train_data, X_train, y_train, X_test, y_test = get_train_test_datasets(X_ne_train, y_ne_train, X_ne_test, y_ne_test)\n",
        "trainloader = torch.utils.data.DataLoader(train_data, batch_size=4, shuffle=True, num_workers=1)\n",
        "\n",
        "# Initialize the MLP\n",
        "mlp = MLP()\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "loss_function = nn.L1Loss()\n",
        "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "# Run the training loop\n",
        "for epoch in range(0, 200): # 5 epochs at maximum\n",
        "  \n",
        "  # Print epoch\n",
        "  print(f'Starting epoch {epoch+1}')\n",
        "  \n",
        "  # Set current loss value\n",
        "  current_loss = 0.0\n",
        "  \n",
        "  # Iterate over the DataLoader for training data\n",
        "  for i, data in enumerate(trainloader, 0):\n",
        "    \n",
        "    # Get and prepare inputs\n",
        "    inputs, targets = data\n",
        "    inputs, targets = inputs.float(), targets.float()\n",
        "    targets = targets.reshape((targets.shape[0], 1))\n",
        "    \n",
        "    # Zero the gradients\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # Perform forward pass\n",
        "    outputs = mlp(inputs)\n",
        "    \n",
        "    # Compute loss\n",
        "    loss = mae_loss(outputs, targets)\n",
        "    \n",
        "    # Perform backward pass\n",
        "    loss.backward()\n",
        "    \n",
        "    # Perform optimization\n",
        "    optimizer.step()\n",
        "    \n",
        "    # Print statistics\n",
        "    current_loss += loss.item()\n",
        "    if i % 100 == 0:\n",
        "        print('Loss after mini-batch %5d: %.3f' %\n",
        "              (i + 1, current_loss / 500))\n",
        "        current_loss = 0.0\n",
        "\n",
        "# Process is complete.\n",
        "print('Training process has finished.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1\n",
            "Loss after mini-batch     1: 0.013\n",
            "Loss after mini-batch   101: 1.271\n",
            "Loss after mini-batch   201: 1.242\n",
            "Starting epoch 2\n",
            "Loss after mini-batch     1: 0.011\n",
            "Loss after mini-batch   101: 1.093\n",
            "Loss after mini-batch   201: 0.954\n",
            "Starting epoch 3\n",
            "Loss after mini-batch     1: 0.005\n",
            "Loss after mini-batch   101: 0.662\n",
            "Loss after mini-batch   201: 0.503\n",
            "Starting epoch 4\n",
            "Loss after mini-batch     1: 0.006\n",
            "Loss after mini-batch   101: 0.399\n",
            "Loss after mini-batch   201: 0.376\n",
            "Starting epoch 5\n",
            "Loss after mini-batch     1: 0.006\n",
            "Loss after mini-batch   101: 0.348\n",
            "Loss after mini-batch   201: 0.348\n",
            "Starting epoch 6\n",
            "Loss after mini-batch     1: 0.006\n",
            "Loss after mini-batch   101: 0.341\n",
            "Loss after mini-batch   201: 0.303\n",
            "Starting epoch 7\n",
            "Loss after mini-batch     1: 0.005\n",
            "Loss after mini-batch   101: 0.301\n",
            "Loss after mini-batch   201: 0.295\n",
            "Starting epoch 8\n",
            "Loss after mini-batch     1: 0.004\n",
            "Loss after mini-batch   101: 0.310\n",
            "Loss after mini-batch   201: 0.309\n",
            "Starting epoch 9\n",
            "Loss after mini-batch     1: 0.004\n",
            "Loss after mini-batch   101: 0.286\n",
            "Loss after mini-batch   201: 0.278\n",
            "Starting epoch 10\n",
            "Loss after mini-batch     1: 0.003\n",
            "Loss after mini-batch   101: 0.300\n",
            "Loss after mini-batch   201: 0.273\n",
            "Starting epoch 11\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.265\n",
            "Loss after mini-batch   201: 0.283\n",
            "Starting epoch 12\n",
            "Loss after mini-batch     1: 0.003\n",
            "Loss after mini-batch   101: 0.282\n",
            "Loss after mini-batch   201: 0.268\n",
            "Starting epoch 13\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.277\n",
            "Loss after mini-batch   201: 0.270\n",
            "Starting epoch 14\n",
            "Loss after mini-batch     1: 0.004\n",
            "Loss after mini-batch   101: 0.248\n",
            "Loss after mini-batch   201: 0.260\n",
            "Starting epoch 15\n",
            "Loss after mini-batch     1: 0.003\n",
            "Loss after mini-batch   101: 0.266\n",
            "Loss after mini-batch   201: 0.246\n",
            "Starting epoch 16\n",
            "Loss after mini-batch     1: 0.003\n",
            "Loss after mini-batch   101: 0.232\n",
            "Loss after mini-batch   201: 0.244\n",
            "Starting epoch 17\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.254\n",
            "Loss after mini-batch   201: 0.225\n",
            "Starting epoch 18\n",
            "Loss after mini-batch     1: 0.004\n",
            "Loss after mini-batch   101: 0.228\n",
            "Loss after mini-batch   201: 0.243\n",
            "Starting epoch 19\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.244\n",
            "Loss after mini-batch   201: 0.226\n",
            "Starting epoch 20\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.249\n",
            "Loss after mini-batch   201: 0.239\n",
            "Starting epoch 21\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.229\n",
            "Loss after mini-batch   201: 0.222\n",
            "Starting epoch 22\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.228\n",
            "Loss after mini-batch   201: 0.206\n",
            "Starting epoch 23\n",
            "Loss after mini-batch     1: 0.003\n",
            "Loss after mini-batch   101: 0.211\n",
            "Loss after mini-batch   201: 0.223\n",
            "Starting epoch 24\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.209\n",
            "Loss after mini-batch   201: 0.219\n",
            "Starting epoch 25\n",
            "Loss after mini-batch     1: 0.004\n",
            "Loss after mini-batch   101: 0.209\n",
            "Loss after mini-batch   201: 0.218\n",
            "Starting epoch 26\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.202\n",
            "Loss after mini-batch   201: 0.201\n",
            "Starting epoch 27\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.208\n",
            "Loss after mini-batch   201: 0.203\n",
            "Starting epoch 28\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.205\n",
            "Loss after mini-batch   201: 0.206\n",
            "Starting epoch 29\n",
            "Loss after mini-batch     1: 0.004\n",
            "Loss after mini-batch   101: 0.192\n",
            "Loss after mini-batch   201: 0.206\n",
            "Starting epoch 30\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.200\n",
            "Loss after mini-batch   201: 0.183\n",
            "Starting epoch 31\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.197\n",
            "Loss after mini-batch   201: 0.195\n",
            "Starting epoch 32\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.193\n",
            "Loss after mini-batch   201: 0.175\n",
            "Starting epoch 33\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.201\n",
            "Loss after mini-batch   201: 0.192\n",
            "Starting epoch 34\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.182\n",
            "Loss after mini-batch   201: 0.191\n",
            "Starting epoch 35\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.191\n",
            "Loss after mini-batch   201: 0.180\n",
            "Starting epoch 36\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.185\n",
            "Loss after mini-batch   201: 0.184\n",
            "Starting epoch 37\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.178\n",
            "Loss after mini-batch   201: 0.174\n",
            "Starting epoch 38\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.182\n",
            "Loss after mini-batch   201: 0.175\n",
            "Starting epoch 39\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.170\n",
            "Loss after mini-batch   201: 0.171\n",
            "Starting epoch 40\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.164\n",
            "Loss after mini-batch   201: 0.163\n",
            "Starting epoch 41\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.187\n",
            "Loss after mini-batch   201: 0.176\n",
            "Starting epoch 42\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.175\n",
            "Loss after mini-batch   201: 0.170\n",
            "Starting epoch 43\n",
            "Loss after mini-batch     1: 0.005\n",
            "Loss after mini-batch   101: 0.169\n",
            "Loss after mini-batch   201: 0.165\n",
            "Starting epoch 44\n",
            "Loss after mini-batch     1: 0.003\n",
            "Loss after mini-batch   101: 0.162\n",
            "Loss after mini-batch   201: 0.173\n",
            "Starting epoch 45\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.151\n",
            "Loss after mini-batch   201: 0.165\n",
            "Starting epoch 46\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.160\n",
            "Loss after mini-batch   201: 0.163\n",
            "Starting epoch 47\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.152\n",
            "Loss after mini-batch   201: 0.155\n",
            "Starting epoch 48\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.161\n",
            "Loss after mini-batch   201: 0.148\n",
            "Starting epoch 49\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.151\n",
            "Loss after mini-batch   201: 0.158\n",
            "Starting epoch 50\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.163\n",
            "Loss after mini-batch   201: 0.152\n",
            "Starting epoch 51\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.155\n",
            "Loss after mini-batch   201: 0.158\n",
            "Starting epoch 52\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.145\n",
            "Loss after mini-batch   201: 0.151\n",
            "Starting epoch 53\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.151\n",
            "Loss after mini-batch   201: 0.145\n",
            "Starting epoch 54\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.146\n",
            "Loss after mini-batch   201: 0.156\n",
            "Starting epoch 55\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.138\n",
            "Loss after mini-batch   201: 0.145\n",
            "Starting epoch 56\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.149\n",
            "Loss after mini-batch   201: 0.154\n",
            "Starting epoch 57\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.141\n",
            "Loss after mini-batch   201: 0.145\n",
            "Starting epoch 58\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.141\n",
            "Loss after mini-batch   201: 0.137\n",
            "Starting epoch 59\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.133\n",
            "Loss after mini-batch   201: 0.138\n",
            "Starting epoch 60\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.141\n",
            "Loss after mini-batch   201: 0.137\n",
            "Starting epoch 61\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.142\n",
            "Loss after mini-batch   201: 0.147\n",
            "Starting epoch 62\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.143\n",
            "Loss after mini-batch   201: 0.139\n",
            "Starting epoch 63\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.136\n",
            "Loss after mini-batch   201: 0.146\n",
            "Starting epoch 64\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.139\n",
            "Loss after mini-batch   201: 0.130\n",
            "Starting epoch 65\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.125\n",
            "Loss after mini-batch   201: 0.130\n",
            "Starting epoch 66\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.127\n",
            "Loss after mini-batch   201: 0.132\n",
            "Starting epoch 67\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.126\n",
            "Loss after mini-batch   201: 0.134\n",
            "Starting epoch 68\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.134\n",
            "Loss after mini-batch   201: 0.129\n",
            "Starting epoch 69\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.128\n",
            "Loss after mini-batch   201: 0.126\n",
            "Starting epoch 70\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.119\n",
            "Loss after mini-batch   201: 0.132\n",
            "Starting epoch 71\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.127\n",
            "Loss after mini-batch   201: 0.130\n",
            "Starting epoch 72\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.128\n",
            "Loss after mini-batch   201: 0.121\n",
            "Starting epoch 73\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.124\n",
            "Loss after mini-batch   201: 0.124\n",
            "Starting epoch 74\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.119\n",
            "Loss after mini-batch   201: 0.116\n",
            "Starting epoch 75\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.120\n",
            "Loss after mini-batch   201: 0.125\n",
            "Starting epoch 76\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.123\n",
            "Loss after mini-batch   201: 0.125\n",
            "Starting epoch 77\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.127\n",
            "Loss after mini-batch   201: 0.115\n",
            "Starting epoch 78\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.122\n",
            "Loss after mini-batch   201: 0.119\n",
            "Starting epoch 79\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.117\n",
            "Loss after mini-batch   201: 0.119\n",
            "Starting epoch 80\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.112\n",
            "Loss after mini-batch   201: 0.123\n",
            "Starting epoch 81\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.115\n",
            "Loss after mini-batch   201: 0.116\n",
            "Starting epoch 82\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch   101: 0.117\n",
            "Loss after mini-batch   201: 0.120\n",
            "Starting epoch 83\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.116\n",
            "Loss after mini-batch   201: 0.112\n",
            "Starting epoch 84\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.121\n",
            "Loss after mini-batch   201: 0.115\n",
            "Starting epoch 85\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.112\n",
            "Loss after mini-batch   201: 0.113\n",
            "Starting epoch 86\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.116\n",
            "Loss after mini-batch   201: 0.120\n",
            "Starting epoch 87\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.108\n",
            "Loss after mini-batch   201: 0.110\n",
            "Starting epoch 88\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.111\n",
            "Loss after mini-batch   201: 0.114\n",
            "Starting epoch 89\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.112\n",
            "Loss after mini-batch   201: 0.113\n",
            "Starting epoch 90\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.110\n",
            "Loss after mini-batch   201: 0.112\n",
            "Starting epoch 91\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.110\n",
            "Loss after mini-batch   201: 0.110\n",
            "Starting epoch 92\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.110\n",
            "Loss after mini-batch   201: 0.107\n",
            "Starting epoch 93\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.111\n",
            "Loss after mini-batch   201: 0.114\n",
            "Starting epoch 94\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.110\n",
            "Loss after mini-batch   201: 0.106\n",
            "Starting epoch 95\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.106\n",
            "Loss after mini-batch   201: 0.110\n",
            "Starting epoch 96\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.113\n",
            "Loss after mini-batch   201: 0.108\n",
            "Starting epoch 97\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.111\n",
            "Loss after mini-batch   201: 0.105\n",
            "Starting epoch 98\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.103\n",
            "Loss after mini-batch   201: 0.116\n",
            "Starting epoch 99\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.105\n",
            "Loss after mini-batch   201: 0.107\n",
            "Starting epoch 100\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.110\n",
            "Loss after mini-batch   201: 0.109\n",
            "Starting epoch 101\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.104\n",
            "Loss after mini-batch   201: 0.110\n",
            "Starting epoch 102\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.101\n",
            "Loss after mini-batch   201: 0.106\n",
            "Starting epoch 103\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.101\n",
            "Loss after mini-batch   201: 0.099\n",
            "Starting epoch 104\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.111\n",
            "Loss after mini-batch   201: 0.104\n",
            "Starting epoch 105\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.098\n",
            "Loss after mini-batch   201: 0.105\n",
            "Starting epoch 106\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.106\n",
            "Loss after mini-batch   201: 0.100\n",
            "Starting epoch 107\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.100\n",
            "Loss after mini-batch   201: 0.099\n",
            "Starting epoch 108\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.098\n",
            "Loss after mini-batch   201: 0.099\n",
            "Starting epoch 109\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.102\n",
            "Loss after mini-batch   201: 0.101\n",
            "Starting epoch 110\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.101\n",
            "Loss after mini-batch   201: 0.098\n",
            "Starting epoch 111\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.106\n",
            "Loss after mini-batch   201: 0.103\n",
            "Starting epoch 112\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.102\n",
            "Loss after mini-batch   201: 0.099\n",
            "Starting epoch 113\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.098\n",
            "Loss after mini-batch   201: 0.101\n",
            "Starting epoch 114\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.099\n",
            "Loss after mini-batch   201: 0.100\n",
            "Starting epoch 115\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.100\n",
            "Loss after mini-batch   201: 0.106\n",
            "Starting epoch 116\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.110\n",
            "Loss after mini-batch   201: 0.098\n",
            "Starting epoch 117\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.108\n",
            "Loss after mini-batch   201: 0.097\n",
            "Starting epoch 118\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.092\n",
            "Loss after mini-batch   201: 0.104\n",
            "Starting epoch 119\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.101\n",
            "Loss after mini-batch   201: 0.097\n",
            "Starting epoch 120\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.101\n",
            "Loss after mini-batch   201: 0.097\n",
            "Starting epoch 121\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.102\n",
            "Loss after mini-batch   201: 0.097\n",
            "Starting epoch 122\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.097\n",
            "Loss after mini-batch   201: 0.100\n",
            "Starting epoch 123\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.098\n",
            "Loss after mini-batch   201: 0.098\n",
            "Starting epoch 124\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.097\n",
            "Loss after mini-batch   201: 0.102\n",
            "Starting epoch 125\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.100\n",
            "Loss after mini-batch   201: 0.096\n",
            "Starting epoch 126\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.102\n",
            "Loss after mini-batch   201: 0.094\n",
            "Starting epoch 127\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch   101: 0.097\n",
            "Loss after mini-batch   201: 0.096\n",
            "Starting epoch 128\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.101\n",
            "Loss after mini-batch   201: 0.100\n",
            "Starting epoch 129\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.100\n",
            "Loss after mini-batch   201: 0.096\n",
            "Starting epoch 130\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.100\n",
            "Loss after mini-batch   201: 0.101\n",
            "Starting epoch 131\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch   101: 0.095\n",
            "Loss after mini-batch   201: 0.099\n",
            "Starting epoch 132\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.098\n",
            "Loss after mini-batch   201: 0.097\n",
            "Starting epoch 133\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.096\n",
            "Loss after mini-batch   201: 0.101\n",
            "Starting epoch 134\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.098\n",
            "Loss after mini-batch   201: 0.107\n",
            "Starting epoch 135\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.098\n",
            "Loss after mini-batch   201: 0.096\n",
            "Starting epoch 136\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.096\n",
            "Loss after mini-batch   201: 0.097\n",
            "Starting epoch 137\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.093\n",
            "Loss after mini-batch   201: 0.095\n",
            "Starting epoch 138\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.098\n",
            "Loss after mini-batch   201: 0.096\n",
            "Starting epoch 139\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.099\n",
            "Loss after mini-batch   201: 0.100\n",
            "Starting epoch 140\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch   101: 0.098\n",
            "Loss after mini-batch   201: 0.100\n",
            "Starting epoch 141\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch   101: 0.103\n",
            "Loss after mini-batch   201: 0.096\n",
            "Starting epoch 142\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.090\n",
            "Loss after mini-batch   201: 0.096\n",
            "Starting epoch 143\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.098\n",
            "Loss after mini-batch   201: 0.098\n",
            "Starting epoch 144\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.099\n",
            "Loss after mini-batch   201: 0.098\n",
            "Starting epoch 145\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.102\n",
            "Loss after mini-batch   201: 0.097\n",
            "Starting epoch 146\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch   101: 0.098\n",
            "Loss after mini-batch   201: 0.100\n",
            "Starting epoch 147\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.092\n",
            "Loss after mini-batch   201: 0.100\n",
            "Starting epoch 148\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.099\n",
            "Loss after mini-batch   201: 0.096\n",
            "Starting epoch 149\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.098\n",
            "Loss after mini-batch   201: 0.098\n",
            "Starting epoch 150\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.095\n",
            "Loss after mini-batch   201: 0.096\n",
            "Starting epoch 151\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.092\n",
            "Loss after mini-batch   201: 0.098\n",
            "Starting epoch 152\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.106\n",
            "Loss after mini-batch   201: 0.096\n",
            "Starting epoch 153\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.098\n",
            "Loss after mini-batch   201: 0.099\n",
            "Starting epoch 154\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.097\n",
            "Loss after mini-batch   201: 0.099\n",
            "Starting epoch 155\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.096\n",
            "Loss after mini-batch   201: 0.098\n",
            "Starting epoch 156\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.095\n",
            "Loss after mini-batch   201: 0.093\n",
            "Starting epoch 157\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.098\n",
            "Loss after mini-batch   201: 0.097\n",
            "Starting epoch 158\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.098\n",
            "Loss after mini-batch   201: 0.096\n",
            "Starting epoch 159\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.090\n",
            "Loss after mini-batch   201: 0.097\n",
            "Starting epoch 160\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.095\n",
            "Loss after mini-batch   201: 0.099\n",
            "Starting epoch 161\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.092\n",
            "Loss after mini-batch   201: 0.095\n",
            "Starting epoch 162\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.098\n",
            "Loss after mini-batch   201: 0.093\n",
            "Starting epoch 163\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.091\n",
            "Loss after mini-batch   201: 0.096\n",
            "Starting epoch 164\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.096\n",
            "Loss after mini-batch   201: 0.091\n",
            "Starting epoch 165\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.091\n",
            "Loss after mini-batch   201: 0.099\n",
            "Starting epoch 166\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.096\n",
            "Loss after mini-batch   201: 0.095\n",
            "Starting epoch 167\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.093\n",
            "Loss after mini-batch   201: 0.093\n",
            "Starting epoch 168\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.092\n",
            "Loss after mini-batch   201: 0.099\n",
            "Starting epoch 169\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.095\n",
            "Loss after mini-batch   201: 0.093\n",
            "Starting epoch 170\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.094\n",
            "Loss after mini-batch   201: 0.095\n",
            "Starting epoch 171\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.098\n",
            "Loss after mini-batch   201: 0.088\n",
            "Starting epoch 172\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.096\n",
            "Loss after mini-batch   201: 0.092\n",
            "Starting epoch 173\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.092\n",
            "Loss after mini-batch   201: 0.099\n",
            "Starting epoch 174\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.092\n",
            "Loss after mini-batch   201: 0.096\n",
            "Starting epoch 175\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.099\n",
            "Loss after mini-batch   201: 0.091\n",
            "Starting epoch 176\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.093\n",
            "Loss after mini-batch   201: 0.100\n",
            "Starting epoch 177\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.094\n",
            "Loss after mini-batch   201: 0.094\n",
            "Starting epoch 178\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.086\n",
            "Loss after mini-batch   201: 0.104\n",
            "Starting epoch 179\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.092\n",
            "Loss after mini-batch   201: 0.096\n",
            "Starting epoch 180\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.096\n",
            "Loss after mini-batch   201: 0.092\n",
            "Starting epoch 181\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.097\n",
            "Loss after mini-batch   201: 0.090\n",
            "Starting epoch 182\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch   101: 0.098\n",
            "Loss after mini-batch   201: 0.093\n",
            "Starting epoch 183\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.094\n",
            "Loss after mini-batch   201: 0.094\n",
            "Starting epoch 184\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.097\n",
            "Loss after mini-batch   201: 0.092\n",
            "Starting epoch 185\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch   101: 0.096\n",
            "Loss after mini-batch   201: 0.095\n",
            "Starting epoch 186\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.095\n",
            "Loss after mini-batch   201: 0.093\n",
            "Starting epoch 187\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.093\n",
            "Loss after mini-batch   201: 0.096\n",
            "Starting epoch 188\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.096\n",
            "Loss after mini-batch   201: 0.089\n",
            "Starting epoch 189\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.090\n",
            "Loss after mini-batch   201: 0.095\n",
            "Starting epoch 190\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.085\n",
            "Loss after mini-batch   201: 0.096\n",
            "Starting epoch 191\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.097\n",
            "Loss after mini-batch   201: 0.092\n",
            "Starting epoch 192\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.090\n",
            "Loss after mini-batch   201: 0.100\n",
            "Starting epoch 193\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.096\n",
            "Loss after mini-batch   201: 0.096\n",
            "Starting epoch 194\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.094\n",
            "Loss after mini-batch   201: 0.087\n",
            "Starting epoch 195\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.093\n",
            "Loss after mini-batch   201: 0.086\n",
            "Starting epoch 196\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.089\n",
            "Loss after mini-batch   201: 0.099\n",
            "Starting epoch 197\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.092\n",
            "Loss after mini-batch   201: 0.091\n",
            "Starting epoch 198\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.092\n",
            "Loss after mini-batch   201: 0.094\n",
            "Starting epoch 199\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.091\n",
            "Loss after mini-batch   201: 0.094\n",
            "Starting epoch 200\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch   101: 0.092\n",
            "Loss after mini-batch   201: 0.091\n",
            "Training process has finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChmakS1iJb6a",
        "outputId": "759eae42-29b7-4fa1-98bc-fc1fe44445e6"
      },
      "source": [
        "# Train RMSE\n",
        "\n",
        "mlp.eval()\n",
        "y_preds = mlp(torch.tensor(StandardScaler().fit_transform(X_train)).float())\n",
        "y_preds = y_preds.detach().numpy().reshape(y_preds.shape[0])\n",
        "print(y_preds.shape)\n",
        "print(torch.tensor(y_train).float())\n",
        "print(get_rmse(y_preds, y_train))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1161,)\n",
            "tensor([6.7758, 7.3979, 7.4826,  ..., 5.4723, 5.7397, 6.6798])\n",
            "0.5663755457230754\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pu9BnToGJb8b",
        "outputId": "9a04433a-3c71-4ce4-f8dc-18bb98b0a97c"
      },
      "source": [
        "# Test RMSE\n",
        "\n",
        "y_pred_test = mlp(torch.tensor(StandardScaler().fit_transform(X_test)).float())\n",
        "y_pred_test = y_pred_test.detach().numpy().reshape(y_pred_test.shape[0])\n",
        "print(y_pred_test.shape)\n",
        "print(get_rmse(y_pred_test, y_test.numpy()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(129,)\n",
            "0.5352834947772126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIujhzUuKYJS"
      },
      "source": [
        "### Best - No embeddings\n",
        "\n",
        "Train RMSE = 0.5663755457230754\n",
        "\n",
        "Test RMSE = 0.5352834947772126"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eb-yeo5Jw_e"
      },
      "source": [
        "# Best settings below \n",
        "\n",
        "def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "      nn.Linear(13, 64), # 11\n",
        "      nn.Dropout(),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(64, 32),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(32, 1)\n",
        "    )\n",
        "\n",
        "mae_loss\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(train_data, batch_size=4, shuffle=True, num_workers=1)\n",
        "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "for epoch in range(0, 200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pYFy2xH_ap0"
      },
      "source": [
        "## Hybrid Approach (MLP + CNN Embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NcKJPRBLdeR"
      },
      "source": [
        "# ALL EMBEDDINGS\n",
        "resnet_ct = 1000\n",
        "clip_ct = 512\n",
        "\n",
        "feature_resnet_lst = ['year', 'tmdb_budget', 'tmdb_revenue', 'imdb_revenue', 'tmdb_runtime', 'imdb_runtime', \n",
        "                     'tm_rating', 'tm_count', 'tm_top_critics_count', 'tm_fresh_critics_count', \n",
        "                     'tm_rotten_critics_count', 'ebert_rating', 'imdb_budget']\n",
        "feature_clip_lst = ['year', 'tmdb_budget', 'tmdb_revenue', 'imdb_revenue', 'tmdb_runtime', 'imdb_runtime', \n",
        "                     'tm_rating', 'tm_count', 'tm_top_critics_count', 'tm_fresh_critics_count', \n",
        "                     'tm_rotten_critics_count', 'ebert_rating', 'imdb_budget']\n",
        "for i in range(1, resnet_ct + 1):\n",
        "    feature_resnet_lst.append('resnet-' + str(i))\n",
        "for i in range(1, clip_ct + 1):\n",
        "    feature_clip_lst.append('clip-' + str(i))\n",
        "\n",
        "output_label = 'boxd_vote_average'\n",
        "\n",
        "extract_resnet_lst = feature_resnet_lst + [output_label]\n",
        "extract_clip_lst = feature_clip_lst + [output_label]\n",
        "\n",
        "X_resnet_all = movies_embeddings_df[extract_resnet_lst]\n",
        "X_clip_all = movies_embeddings_df[extract_clip_lst]\n",
        "\n",
        "print(get_num_missing(X_resnet_all))\n",
        "\n",
        "X_resnet = remove_missing_values(X_resnet_all)\n",
        "X_clip = remove_missing_values(X_clip_all)\n",
        "\n",
        "print(X_resnet.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-aDv0bPDvW9"
      },
      "source": [
        "# SVD -- Dimension reduction\n",
        "resnet_svd_ct = 25 # from 1000\n",
        "clip_svd_ct = 100 # from 512\n",
        "\n",
        "feature_resnet_lst = ['year', 'tmdb_budget', 'tmdb_revenue', 'imdb_revenue', 'tmdb_runtime', 'imdb_runtime', \n",
        "                     'tm_rating', 'tm_count', 'tm_top_critics_count', 'tm_fresh_critics_count', \n",
        "                     'tm_rotten_critics_count', 'ebert_rating', 'imdb_budget']\n",
        "feature_clip_lst = ['year', 'tmdb_budget', 'tmdb_revenue', 'imdb_revenue', 'tmdb_runtime', 'imdb_runtime', \n",
        "                     'tm_rating', 'tm_count', 'tm_top_critics_count', 'tm_fresh_critics_count', \n",
        "                     'tm_rotten_critics_count', 'ebert_rating', 'imdb_budget']\n",
        "for i in range(1, resnet_svd_ct + 1):\n",
        "    feature_resnet_lst.append('resnet-svd-' + str(i))\n",
        "for i in range(1, clip_svd_ct + 1):\n",
        "    feature_clip_lst.append('clip-svd-' + str(i))\n",
        "\n",
        "output_label = 'boxd_vote_average'\n",
        "\n",
        "extract_resnet_lst = feature_resnet_lst + [output_label]\n",
        "extract_clip_lst = feature_clip_lst + [output_label]\n",
        "\n",
        "X_resnet_all = movies_svd_df[extract_resnet_lst]\n",
        "X_clip_all = movies_svd_df[extract_clip_lst]\n",
        "\n",
        "print(get_num_missing(X_resnet_all))\n",
        "\n",
        "X_resnet = remove_missing_values(X_resnet_all)\n",
        "X_clip = remove_missing_values(X_clip_all)\n",
        "\n",
        "print(X_resnet.shape)\n",
        "print(X_clip.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWXtfl5-_edQ"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "features_resnet = X_resnet[feature_resnet_lst].to_numpy()\n",
        "outputs_resnet = X_resnet[output_label].to_numpy()\n",
        "X_rn, y_rn = shuffle(features_resnet, outputs_resnet, random_state=0)\n",
        "print(X_rn.shape)\n",
        "print(y_rn.shape)\n",
        "\n",
        "features_clip = X_clip[feature_clip_lst].to_numpy()\n",
        "outputs_clip = X_clip[output_label].to_numpy()\n",
        "X_cp, y_cp = shuffle(features_clip, outputs_clip, random_state=0)\n",
        "print(X_cp.shape)\n",
        "print(y_cp.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GW9TF8YQ-2sn"
      },
      "source": [
        "X_rn_train, y_rn_train, X_rn_test, y_rn_test = train_test_split(X_rn, y_rn, frac=0.9, verbose=True)\n",
        "X_cp_train, y_cp_train, X_cp_test, y_cp_test = train_test_split(X_cp, y_cp, frac=0.9, verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iW5Xk5KrBcwx"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def get_rmse(y_pred, y_true):\n",
        "    return np.sqrt(np.mean((y_pred-y_true)**2))\n",
        "\n",
        "#this model learns to minimize MAE\n",
        "def mae_loss(y_pred, y_true):\n",
        "    mae = torch.abs(y_true - y_pred).mean()\n",
        "    return mae\n",
        "\n",
        "#this model learns to minimize RMSE\n",
        "def rmse_loss(y_pred, y_true):\n",
        "    return torch.sqrt(torch.mean((y_pred-y_true)**2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKPEa_N0Eev2"
      },
      "source": [
        "def get_train_test_datasets(X_train, y_train, X_test, y_test):\n",
        "    return MovieDataset(X_train, y_train), X_train, y_train, torch.tensor(X_test), torch.tensor(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DglwkG0DJEx"
      },
      "source": [
        "class MovieDataset(torch.utils.data.Dataset):\n",
        "  '''\n",
        "  Prepare the Movie dataset for regression\n",
        "  '''\n",
        "\n",
        "  def __init__(self, X, y, scale_data=True):\n",
        "    if not torch.is_tensor(X) and not torch.is_tensor(y):\n",
        "      # Apply scaling if necessary\n",
        "      if scale_data:\n",
        "          X = StandardScaler().fit_transform(X)\n",
        "      self.X = torch.from_numpy(X)\n",
        "      self.y = torch.from_numpy(y)\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.X)\n",
        "\n",
        "  def __getitem__(self, i):\n",
        "      return self.X[i], self.y[i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JuC0LC3B6ey"
      },
      "source": [
        "class MLP(nn.Module):\n",
        "  '''\n",
        "    Multilayer Perceptron for regression.\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "      nn.Linear(38, 128), # 1011 vs 523 OR 36 vs 111\n",
        "      nn.Dropout(),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(128, 64),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(64, 1)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''\n",
        "      Forward pass\n",
        "    '''\n",
        "    return self.layers(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jL8SmSjDoh2",
        "outputId": "2af00fa6-6445-4485-e0a2-9a6c5d86e006"
      },
      "source": [
        "# Set fixed random number seed\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Flip between ResNet and ClipNet embeddings\n",
        "train_data, X_train, y_train, X_test, y_test = get_train_test_datasets(X_rn_train, y_rn_train, X_rn_test, y_rn_test)\n",
        "# train_data, X_train, y_train, X_test, y_test = get_train_test_datasets(X_cp_train, y_cp_train, X_cp_test, y_cp_test)\n",
        "trainloader = torch.utils.data.DataLoader(train_data, batch_size=4, shuffle=True, num_workers=1)\n",
        "\n",
        "# Initialize the MLP\n",
        "mlp = MLP()\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "loss_function = nn.L1Loss()\n",
        "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "# Run the training loop\n",
        "for epoch in range(0, 200): # 5 epochs at maximum\n",
        "  \n",
        "  # Print epoch\n",
        "  print(f'Starting epoch {epoch+1}')\n",
        "  \n",
        "  # Set current loss value\n",
        "  current_loss = 0.0\n",
        "  \n",
        "  # Iterate over the DataLoader for training data\n",
        "  for i, data in enumerate(trainloader, 0):\n",
        "    \n",
        "    # Get and prepare inputs\n",
        "    inputs, targets = data\n",
        "    inputs, targets = inputs.float(), targets.float()\n",
        "    targets = targets.reshape((targets.shape[0], 1))\n",
        "    \n",
        "    # Zero the gradients\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # Perform forward pass\n",
        "    outputs = mlp(inputs)\n",
        "    \n",
        "    # Compute loss\n",
        "    loss = mae_loss(outputs, targets)\n",
        "  \n",
        "    # Perform backward pass\n",
        "    loss.backward()\n",
        "    \n",
        "    # Perform optimization\n",
        "    optimizer.step()\n",
        "    \n",
        "    # Print statistics\n",
        "    current_loss += loss.item()\n",
        "    if i % 100 == 0:\n",
        "        print('Loss after mini-batch %5d: %.3f' %\n",
        "              (i + 1, current_loss / 500))\n",
        "        current_loss = 0.0\n",
        "\n",
        "# Process is complete.\n",
        "print('Training process has finished.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1\n",
            "Loss after mini-batch     1: 0.014\n",
            "Loss after mini-batch   101: 1.296\n",
            "Loss after mini-batch   201: 1.184\n",
            "Starting epoch 2\n",
            "Loss after mini-batch     1: 0.009\n",
            "Loss after mini-batch   101: 0.841\n",
            "Loss after mini-batch   201: 0.574\n",
            "Starting epoch 3\n",
            "Loss after mini-batch     1: 0.003\n",
            "Loss after mini-batch   101: 0.274\n",
            "Loss after mini-batch   201: 0.255\n",
            "Starting epoch 4\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.241\n",
            "Loss after mini-batch   201: 0.235\n",
            "Starting epoch 5\n",
            "Loss after mini-batch     1: 0.004\n",
            "Loss after mini-batch   101: 0.228\n",
            "Loss after mini-batch   201: 0.234\n",
            "Starting epoch 6\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.214\n",
            "Loss after mini-batch   201: 0.228\n",
            "Starting epoch 7\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.224\n",
            "Loss after mini-batch   201: 0.207\n",
            "Starting epoch 8\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.209\n",
            "Loss after mini-batch   201: 0.222\n",
            "Starting epoch 9\n",
            "Loss after mini-batch     1: 0.004\n",
            "Loss after mini-batch   101: 0.228\n",
            "Loss after mini-batch   201: 0.205\n",
            "Starting epoch 10\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.207\n",
            "Loss after mini-batch   201: 0.205\n",
            "Starting epoch 11\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.212\n",
            "Loss after mini-batch   201: 0.210\n",
            "Starting epoch 12\n",
            "Loss after mini-batch     1: 0.003\n",
            "Loss after mini-batch   101: 0.195\n",
            "Loss after mini-batch   201: 0.204\n",
            "Starting epoch 13\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.201\n",
            "Loss after mini-batch   201: 0.185\n",
            "Starting epoch 14\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.190\n",
            "Loss after mini-batch   201: 0.189\n",
            "Starting epoch 15\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.188\n",
            "Loss after mini-batch   201: 0.207\n",
            "Starting epoch 16\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.189\n",
            "Loss after mini-batch   201: 0.196\n",
            "Starting epoch 17\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.186\n",
            "Loss after mini-batch   201: 0.198\n",
            "Starting epoch 18\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.185\n",
            "Loss after mini-batch   201: 0.188\n",
            "Starting epoch 19\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.189\n",
            "Loss after mini-batch   201: 0.194\n",
            "Starting epoch 20\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.186\n",
            "Loss after mini-batch   201: 0.183\n",
            "Starting epoch 21\n",
            "Loss after mini-batch     1: 0.003\n",
            "Loss after mini-batch   101: 0.201\n",
            "Loss after mini-batch   201: 0.180\n",
            "Starting epoch 22\n",
            "Loss after mini-batch     1: 0.003\n",
            "Loss after mini-batch   101: 0.183\n",
            "Loss after mini-batch   201: 0.169\n",
            "Starting epoch 23\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.185\n",
            "Loss after mini-batch   201: 0.178\n",
            "Starting epoch 24\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.173\n",
            "Loss after mini-batch   201: 0.181\n",
            "Starting epoch 25\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.177\n",
            "Loss after mini-batch   201: 0.171\n",
            "Starting epoch 26\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.167\n",
            "Loss after mini-batch   201: 0.185\n",
            "Starting epoch 27\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.172\n",
            "Loss after mini-batch   201: 0.166\n",
            "Starting epoch 28\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.165\n",
            "Loss after mini-batch   201: 0.173\n",
            "Starting epoch 29\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.173\n",
            "Loss after mini-batch   201: 0.170\n",
            "Starting epoch 30\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.166\n",
            "Loss after mini-batch   201: 0.166\n",
            "Starting epoch 31\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.173\n",
            "Loss after mini-batch   201: 0.164\n",
            "Starting epoch 32\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.162\n",
            "Loss after mini-batch   201: 0.172\n",
            "Starting epoch 33\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.165\n",
            "Loss after mini-batch   201: 0.154\n",
            "Starting epoch 34\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.172\n",
            "Loss after mini-batch   201: 0.165\n",
            "Starting epoch 35\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.153\n",
            "Loss after mini-batch   201: 0.163\n",
            "Starting epoch 36\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.178\n",
            "Loss after mini-batch   201: 0.169\n",
            "Starting epoch 37\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch   101: 0.160\n",
            "Loss after mini-batch   201: 0.165\n",
            "Starting epoch 38\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.157\n",
            "Loss after mini-batch   201: 0.161\n",
            "Starting epoch 39\n",
            "Loss after mini-batch     1: 0.004\n",
            "Loss after mini-batch   101: 0.156\n",
            "Loss after mini-batch   201: 0.148\n",
            "Starting epoch 40\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.161\n",
            "Loss after mini-batch   201: 0.147\n",
            "Starting epoch 41\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.152\n",
            "Loss after mini-batch   201: 0.163\n",
            "Starting epoch 42\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.150\n",
            "Loss after mini-batch   201: 0.143\n",
            "Starting epoch 43\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.155\n",
            "Loss after mini-batch   201: 0.150\n",
            "Starting epoch 44\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.147\n",
            "Loss after mini-batch   201: 0.152\n",
            "Starting epoch 45\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.148\n",
            "Loss after mini-batch   201: 0.153\n",
            "Starting epoch 46\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.150\n",
            "Loss after mini-batch   201: 0.151\n",
            "Starting epoch 47\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.152\n",
            "Loss after mini-batch   201: 0.150\n",
            "Starting epoch 48\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.149\n",
            "Loss after mini-batch   201: 0.145\n",
            "Starting epoch 49\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.142\n",
            "Loss after mini-batch   201: 0.149\n",
            "Starting epoch 50\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.141\n",
            "Loss after mini-batch   201: 0.151\n",
            "Starting epoch 51\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.145\n",
            "Loss after mini-batch   201: 0.146\n",
            "Starting epoch 52\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.141\n",
            "Loss after mini-batch   201: 0.144\n",
            "Starting epoch 53\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.146\n",
            "Loss after mini-batch   201: 0.141\n",
            "Starting epoch 54\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.138\n",
            "Loss after mini-batch   201: 0.136\n",
            "Starting epoch 55\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.145\n",
            "Loss after mini-batch   201: 0.135\n",
            "Starting epoch 56\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.147\n",
            "Loss after mini-batch   201: 0.130\n",
            "Starting epoch 57\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.138\n",
            "Loss after mini-batch   201: 0.134\n",
            "Starting epoch 58\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.137\n",
            "Loss after mini-batch   201: 0.131\n",
            "Starting epoch 59\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.140\n",
            "Loss after mini-batch   201: 0.132\n",
            "Starting epoch 60\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.137\n",
            "Loss after mini-batch   201: 0.131\n",
            "Starting epoch 61\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.131\n",
            "Loss after mini-batch   201: 0.129\n",
            "Starting epoch 62\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.127\n",
            "Loss after mini-batch   201: 0.127\n",
            "Starting epoch 63\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch   101: 0.132\n",
            "Loss after mini-batch   201: 0.136\n",
            "Starting epoch 64\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.133\n",
            "Loss after mini-batch   201: 0.117\n",
            "Starting epoch 65\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.126\n",
            "Loss after mini-batch   201: 0.129\n",
            "Starting epoch 66\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.126\n",
            "Loss after mini-batch   201: 0.122\n",
            "Starting epoch 67\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.127\n",
            "Loss after mini-batch   201: 0.126\n",
            "Starting epoch 68\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.118\n",
            "Loss after mini-batch   201: 0.130\n",
            "Starting epoch 69\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.131\n",
            "Loss after mini-batch   201: 0.121\n",
            "Starting epoch 70\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.125\n",
            "Loss after mini-batch   201: 0.119\n",
            "Starting epoch 71\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.116\n",
            "Loss after mini-batch   201: 0.121\n",
            "Starting epoch 72\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.115\n",
            "Loss after mini-batch   201: 0.119\n",
            "Starting epoch 73\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.120\n",
            "Loss after mini-batch   201: 0.121\n",
            "Starting epoch 74\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.115\n",
            "Loss after mini-batch   201: 0.112\n",
            "Starting epoch 75\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.123\n",
            "Loss after mini-batch   201: 0.120\n",
            "Starting epoch 76\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.120\n",
            "Loss after mini-batch   201: 0.114\n",
            "Starting epoch 77\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.111\n",
            "Loss after mini-batch   201: 0.112\n",
            "Starting epoch 78\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.107\n",
            "Loss after mini-batch   201: 0.110\n",
            "Starting epoch 79\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.106\n",
            "Loss after mini-batch   201: 0.113\n",
            "Starting epoch 80\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch   101: 0.116\n",
            "Loss after mini-batch   201: 0.111\n",
            "Starting epoch 81\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.112\n",
            "Loss after mini-batch   201: 0.109\n",
            "Starting epoch 82\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.103\n",
            "Loss after mini-batch   201: 0.114\n",
            "Starting epoch 83\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.114\n",
            "Loss after mini-batch   201: 0.112\n",
            "Starting epoch 84\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.105\n",
            "Loss after mini-batch   201: 0.112\n",
            "Starting epoch 85\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.114\n",
            "Loss after mini-batch   201: 0.109\n",
            "Starting epoch 86\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.104\n",
            "Loss after mini-batch   201: 0.106\n",
            "Starting epoch 87\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.106\n",
            "Loss after mini-batch   201: 0.113\n",
            "Starting epoch 88\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.106\n",
            "Loss after mini-batch   201: 0.105\n",
            "Starting epoch 89\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.103\n",
            "Loss after mini-batch   201: 0.105\n",
            "Starting epoch 90\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.095\n",
            "Loss after mini-batch   201: 0.103\n",
            "Starting epoch 91\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.114\n",
            "Loss after mini-batch   201: 0.104\n",
            "Starting epoch 92\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.103\n",
            "Loss after mini-batch   201: 0.101\n",
            "Starting epoch 93\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.108\n",
            "Loss after mini-batch   201: 0.105\n",
            "Starting epoch 94\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.104\n",
            "Loss after mini-batch   201: 0.106\n",
            "Starting epoch 95\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.103\n",
            "Loss after mini-batch   201: 0.106\n",
            "Starting epoch 96\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.093\n",
            "Loss after mini-batch   201: 0.101\n",
            "Starting epoch 97\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.106\n",
            "Loss after mini-batch   201: 0.103\n",
            "Starting epoch 98\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.102\n",
            "Loss after mini-batch   201: 0.097\n",
            "Starting epoch 99\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.095\n",
            "Loss after mini-batch   201: 0.099\n",
            "Starting epoch 100\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch   101: 0.104\n",
            "Loss after mini-batch   201: 0.098\n",
            "Starting epoch 101\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.097\n",
            "Loss after mini-batch   201: 0.104\n",
            "Starting epoch 102\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.094\n",
            "Loss after mini-batch   201: 0.098\n",
            "Starting epoch 103\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.096\n",
            "Loss after mini-batch   201: 0.101\n",
            "Starting epoch 104\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.091\n",
            "Loss after mini-batch   201: 0.098\n",
            "Starting epoch 105\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch   101: 0.103\n",
            "Loss after mini-batch   201: 0.095\n",
            "Starting epoch 106\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.097\n",
            "Loss after mini-batch   201: 0.100\n",
            "Starting epoch 107\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.092\n",
            "Loss after mini-batch   201: 0.099\n",
            "Starting epoch 108\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.093\n",
            "Loss after mini-batch   201: 0.094\n",
            "Starting epoch 109\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch   101: 0.091\n",
            "Loss after mini-batch   201: 0.096\n",
            "Starting epoch 110\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch   101: 0.093\n",
            "Loss after mini-batch   201: 0.096\n",
            "Starting epoch 111\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch   101: 0.090\n",
            "Loss after mini-batch   201: 0.095\n",
            "Starting epoch 112\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.096\n",
            "Loss after mini-batch   201: 0.086\n",
            "Starting epoch 113\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.095\n",
            "Loss after mini-batch   201: 0.095\n",
            "Starting epoch 114\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.088\n",
            "Loss after mini-batch   201: 0.096\n",
            "Starting epoch 115\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.096\n",
            "Loss after mini-batch   201: 0.094\n",
            "Starting epoch 116\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.101\n",
            "Loss after mini-batch   201: 0.094\n",
            "Starting epoch 117\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch   101: 0.092\n",
            "Loss after mini-batch   201: 0.091\n",
            "Starting epoch 118\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.088\n",
            "Loss after mini-batch   201: 0.098\n",
            "Starting epoch 119\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch   101: 0.093\n",
            "Loss after mini-batch   201: 0.094\n",
            "Starting epoch 120\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.095\n",
            "Loss after mini-batch   201: 0.092\n",
            "Starting epoch 121\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.092\n",
            "Loss after mini-batch   201: 0.096\n",
            "Starting epoch 122\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.093\n",
            "Loss after mini-batch   201: 0.096\n",
            "Starting epoch 123\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.090\n",
            "Loss after mini-batch   201: 0.094\n",
            "Starting epoch 124\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch   101: 0.091\n",
            "Loss after mini-batch   201: 0.086\n",
            "Starting epoch 125\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.086\n",
            "Loss after mini-batch   201: 0.089\n",
            "Starting epoch 126\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.089\n",
            "Loss after mini-batch   201: 0.088\n",
            "Starting epoch 127\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.084\n",
            "Loss after mini-batch   201: 0.089\n",
            "Starting epoch 128\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.090\n",
            "Loss after mini-batch   201: 0.092\n",
            "Starting epoch 129\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.090\n",
            "Loss after mini-batch   201: 0.087\n",
            "Starting epoch 130\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.085\n",
            "Loss after mini-batch   201: 0.093\n",
            "Starting epoch 131\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.088\n",
            "Loss after mini-batch   201: 0.089\n",
            "Starting epoch 132\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.092\n",
            "Loss after mini-batch   201: 0.087\n",
            "Starting epoch 133\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.084\n",
            "Loss after mini-batch   201: 0.096\n",
            "Starting epoch 134\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch   101: 0.087\n",
            "Loss after mini-batch   201: 0.088\n",
            "Starting epoch 135\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.084\n",
            "Loss after mini-batch   201: 0.088\n",
            "Starting epoch 136\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.090\n",
            "Loss after mini-batch   201: 0.086\n",
            "Starting epoch 137\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch   101: 0.083\n",
            "Loss after mini-batch   201: 0.086\n",
            "Starting epoch 138\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.089\n",
            "Loss after mini-batch   201: 0.083\n",
            "Starting epoch 139\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.092\n",
            "Loss after mini-batch   201: 0.090\n",
            "Starting epoch 140\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.084\n",
            "Loss after mini-batch   201: 0.092\n",
            "Starting epoch 141\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.090\n",
            "Loss after mini-batch   201: 0.083\n",
            "Starting epoch 142\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.085\n",
            "Loss after mini-batch   201: 0.086\n",
            "Starting epoch 143\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.084\n",
            "Loss after mini-batch   201: 0.091\n",
            "Starting epoch 144\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.083\n",
            "Loss after mini-batch   201: 0.084\n",
            "Starting epoch 145\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.083\n",
            "Loss after mini-batch   201: 0.090\n",
            "Starting epoch 146\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.089\n",
            "Loss after mini-batch   201: 0.092\n",
            "Starting epoch 147\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.087\n",
            "Loss after mini-batch   201: 0.085\n",
            "Starting epoch 148\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.084\n",
            "Loss after mini-batch   201: 0.087\n",
            "Starting epoch 149\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.087\n",
            "Loss after mini-batch   201: 0.085\n",
            "Starting epoch 150\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch   101: 0.085\n",
            "Loss after mini-batch   201: 0.086\n",
            "Starting epoch 151\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.084\n",
            "Loss after mini-batch   201: 0.087\n",
            "Starting epoch 152\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch   101: 0.085\n",
            "Loss after mini-batch   201: 0.084\n",
            "Starting epoch 153\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.083\n",
            "Loss after mini-batch   201: 0.086\n",
            "Starting epoch 154\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.086\n",
            "Loss after mini-batch   201: 0.086\n",
            "Starting epoch 155\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch   101: 0.081\n",
            "Loss after mini-batch   201: 0.085\n",
            "Starting epoch 156\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.083\n",
            "Loss after mini-batch   201: 0.088\n",
            "Starting epoch 157\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.084\n",
            "Loss after mini-batch   201: 0.079\n",
            "Starting epoch 158\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.082\n",
            "Loss after mini-batch   201: 0.090\n",
            "Starting epoch 159\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch   101: 0.085\n",
            "Loss after mini-batch   201: 0.082\n",
            "Starting epoch 160\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch   101: 0.083\n",
            "Loss after mini-batch   201: 0.086\n",
            "Starting epoch 161\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.091\n",
            "Loss after mini-batch   201: 0.084\n",
            "Starting epoch 162\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.080\n",
            "Loss after mini-batch   201: 0.087\n",
            "Starting epoch 163\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.087\n",
            "Loss after mini-batch   201: 0.085\n",
            "Starting epoch 164\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch   101: 0.084\n",
            "Loss after mini-batch   201: 0.081\n",
            "Starting epoch 165\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch   101: 0.084\n",
            "Loss after mini-batch   201: 0.081\n",
            "Starting epoch 166\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch   101: 0.081\n",
            "Loss after mini-batch   201: 0.082\n",
            "Starting epoch 167\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch   101: 0.081\n",
            "Loss after mini-batch   201: 0.085\n",
            "Starting epoch 168\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.082\n",
            "Loss after mini-batch   201: 0.074\n",
            "Starting epoch 169\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.081\n",
            "Loss after mini-batch   201: 0.089\n",
            "Starting epoch 170\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch   101: 0.085\n",
            "Loss after mini-batch   201: 0.081\n",
            "Starting epoch 171\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.080\n",
            "Loss after mini-batch   201: 0.084\n",
            "Starting epoch 172\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.085\n",
            "Loss after mini-batch   201: 0.083\n",
            "Starting epoch 173\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.086\n",
            "Loss after mini-batch   201: 0.076\n",
            "Starting epoch 174\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.078\n",
            "Loss after mini-batch   201: 0.082\n",
            "Starting epoch 175\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch   101: 0.080\n",
            "Loss after mini-batch   201: 0.076\n",
            "Starting epoch 176\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.081\n",
            "Loss after mini-batch   201: 0.080\n",
            "Starting epoch 177\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.080\n",
            "Loss after mini-batch   201: 0.080\n",
            "Starting epoch 178\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.086\n",
            "Loss after mini-batch   201: 0.077\n",
            "Starting epoch 179\n",
            "Loss after mini-batch     1: 0.002\n",
            "Loss after mini-batch   101: 0.079\n",
            "Loss after mini-batch   201: 0.084\n",
            "Starting epoch 180\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.079\n",
            "Loss after mini-batch   201: 0.086\n",
            "Starting epoch 181\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.081\n",
            "Loss after mini-batch   201: 0.083\n",
            "Starting epoch 182\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.083\n",
            "Loss after mini-batch   201: 0.084\n",
            "Starting epoch 183\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.084\n",
            "Loss after mini-batch   201: 0.078\n",
            "Starting epoch 184\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.080\n",
            "Loss after mini-batch   201: 0.084\n",
            "Starting epoch 185\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.078\n",
            "Loss after mini-batch   201: 0.084\n",
            "Starting epoch 186\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.072\n",
            "Loss after mini-batch   201: 0.084\n",
            "Starting epoch 187\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.082\n",
            "Loss after mini-batch   201: 0.082\n",
            "Starting epoch 188\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.080\n",
            "Loss after mini-batch   201: 0.080\n",
            "Starting epoch 189\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch   101: 0.082\n",
            "Loss after mini-batch   201: 0.075\n",
            "Starting epoch 190\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch   101: 0.078\n",
            "Loss after mini-batch   201: 0.079\n",
            "Starting epoch 191\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch   101: 0.082\n",
            "Loss after mini-batch   201: 0.076\n",
            "Starting epoch 192\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch   101: 0.082\n",
            "Loss after mini-batch   201: 0.077\n",
            "Starting epoch 193\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.079\n",
            "Loss after mini-batch   201: 0.079\n",
            "Starting epoch 194\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.076\n",
            "Loss after mini-batch   201: 0.078\n",
            "Starting epoch 195\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.081\n",
            "Loss after mini-batch   201: 0.079\n",
            "Starting epoch 196\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch   101: 0.082\n",
            "Loss after mini-batch   201: 0.079\n",
            "Starting epoch 197\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.079\n",
            "Loss after mini-batch   201: 0.082\n",
            "Starting epoch 198\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.082\n",
            "Loss after mini-batch   201: 0.074\n",
            "Starting epoch 199\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch   101: 0.073\n",
            "Loss after mini-batch   201: 0.082\n",
            "Starting epoch 200\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch   101: 0.082\n",
            "Loss after mini-batch   201: 0.077\n",
            "Training process has finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoA_rpbkEFaf"
      },
      "source": [
        "# Train RMSE\n",
        "\n",
        "mlp.eval()\n",
        "y_preds = mlp(torch.tensor(StandardScaler().fit_transform(X_train)).float())\n",
        "y_preds = y_preds.detach().numpy().reshape(y_preds.shape[0])\n",
        "print(y_preds.shape)\n",
        "print(torch.tensor(y_train).float())\n",
        "print(get_rmse(y_preds, y_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGHMpfnfPpFh"
      },
      "source": [
        "# Test RMSE\n",
        "\n",
        "y_pred_test = mlp(torch.tensor(StandardScaler().fit_transform(X_test)).float())\n",
        "y_pred_test = y_pred_test.detach().numpy().reshape(y_pred_test.shape[0])\n",
        "print(y_pred_test.shape)\n",
        "print(get_rmse(y_pred_test, y_test.numpy()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDzU3872EaUn"
      },
      "source": [
        "### Best - ResNet-50 (SVD of Embeddings = 25 components)\n",
        "\n",
        "Train RMSE = 0.4218106118942377\n",
        "\n",
        "Test RMSE = 0.48053026270642585"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hd0ipEwEipE"
      },
      "source": [
        "# Best settings\n",
        "\n",
        "self.layers = nn.Sequential(\n",
        "      nn.Linear(38, 128), # 1011 vs 523 OR 36 vs 111\n",
        "      nn.Dropout(),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(128, 64),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(64, 1)\n",
        "    )\n",
        "\n",
        "mae_loss\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(train_data, batch_size=4, shuffle=True, num_workers=1)\n",
        "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "for epoch in range(0, 200): # 5 epochs at maximum"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtshuuOqEgCe"
      },
      "source": [
        "### Best - CLIP (SVD of Embeddings = 100 components)\n",
        "\n",
        "Train RMSE = 0.29143314082098626\n",
        "\n",
        "Test RMSE = 0.5397942532279681"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8L8Y0OYEjA4"
      },
      "source": [
        "# Best settings\n",
        "\n",
        "self.layers = nn.Sequential(\n",
        "      nn.Linear(113, 128), # 36 vs 111\n",
        "      nn.Dropout(),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(128, 32),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(32, 1)\n",
        "    )\n",
        "\n",
        "mae_loss\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(train_data, batch_size=4, shuffle=True, num_workers=1)\n",
        "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "for epoch in range(0, 200): # 5 epochs at maximum"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GhnrfzDY8Wb"
      },
      "source": [
        "### Best - ResNet-50 (1000 embeddings)\n",
        "\n",
        "Train RMSE = 0.5604313187795152\n",
        "\n",
        "Test RMSE = 0.6657604022378522"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVM-XD_GY91z"
      },
      "source": [
        "# Best settings\n",
        "\n",
        "self.layers = nn.Sequential(\n",
        "      nn.Linear(1013, 64), # 1011 vs 523\n",
        "      nn.Dropout(),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(64, 32),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(32, 1)\n",
        "    )\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(train_data, batch_size=4, shuffle=True, num_workers=1)\n",
        "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "for epoch in range(0, 100): # 5 epochs at maximum"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CilwWFN5RjSq"
      },
      "source": [
        "### BEST - CLIP (512 embeddings)\n",
        "\n",
        "Train RMSE = 0.3406810939517073\n",
        "\n",
        "Test RMSE = 0.5907930360256579"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jgq-lVpxRiUi"
      },
      "source": [
        "# Best settings\n",
        "\n",
        "self.layers = nn.Sequential(\n",
        "      nn.Linear(525, 64), # 1011 vs 523\n",
        "      nn.Dropout(),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(64, 32),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(32, 1)\n",
        "    )\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(train_data, batch_size=4, shuffle=True, num_workers=1)\n",
        "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "for epoch in range(0, 100): # 5 epochs at maximum"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}